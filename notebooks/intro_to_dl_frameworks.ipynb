{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open_source_DL_frameworks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "divl7TK36uPv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dF-JFhOG1pyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Open-Source Frameworks for Deep Learning: an Overview\n",
        "\n",
        "\n",
        "This notebook is part of the 2 hrs talk given at the Univerity of Bologna (DISI), Nuovo Campus Universitario, Via Pavese 50, Cesena, FC the 13th of December 2018, 10-12 am. Remember to include the copyright if you want to use, modify or distribute this notebook! :-) Slides of the talk are available [here](https://docs.google.com/presentation/d/1fbTKtp9xOlCL4JtpiLF39x7Vxq2Z-jgn77CTqcBrwEE/edit?usp=sharing).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "R2jITHxjvZx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Abstract** : The rise of deep learning over the last decade has led to profound changes in the landscape of the machine learning software stack both for research and production. In this talk we will provide a comprehensive overview of the *open-source deep learning frameworks* landscape with both a theoretical and hands-on approach. After a brief introduction and historical contextualization, we will highlight common features and distinctions of their recent developments. Finally, we will take at deeper look into three of the most used deep learning frameworks today: *Caffe*, *Tensorflow*, *PyTorch*; with practical examples and considerations worth reckoning in the choice of such libraries.\n",
        "\n",
        "**Short Bio** : [Vincenzo Lomonaco](https://vincenzolomonaco.com) is a Deep Learning PhD student at the University of Bologna and founder of [ContinualAI.org](https://continualai.org). He is also the PhD students representative at the Department of Computer Science of Engineering (DISI) and teaching assistant of the courses *“Machine Learning”* and *“Computer Architectures”* in the same department. Previously, he was a Machine Learning software engineer at IDL in-line Devices and a Master Student at the University of Bologna where he graduated cum laude in 2015 with the dissertation [“Deep Learning for Computer Vision: a Comparison Between CNNs and HTMs on Object Recognition Tasks\"](https://amslaurea.unibo.it/9095/).\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "3pJ2xB48pjgj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Connecting a local runtime**\n",
        "\n",
        "In case resources are not enough for you (no GPU for example), you can always connect another [local runtime](https://research.google.com/colaboratory/local-runtimes.html) or to a [runtime on a Google Compute Engine instance](https://research.google.com/colaboratory/local-runtimes.html). However, this notebook has been designed to run fast enough on simple CPUs so you shouldn't fined any trouble here, using a free *hosted account*.\n",
        "\n",
        "\n",
        "**Requisites to run it locally, outside colab (not recommended)**\n",
        "\n",
        "*   Python 3.x\n",
        "*   Jupyter\n",
        "*   Numpy\n",
        "*   Matplolib\n",
        "*   Pytorch 0.4.0\n",
        "*   Caffe 1.0.0\n",
        "*  Tensorflow 1.12"
      ]
    },
    {
      "metadata": {
        "id": "Bg1m-mau3nim",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hands-on session (45 minutes)\n",
        "\n",
        "In this session we will try to learn an evaluate a Convolutional Neural Networks model on [MNIST](http://yann.lecun.com/exdb/mnist/) using three of the most used deep learning frameworks today: *Caffe*, *Tensorflow*, *PyTorch* with an expected timeframe of 15 minutes for each of them. This will allow us to grasp what it means to train a deep model with such libraries and compare the different Python APIs for this simple use case."
      ]
    },
    {
      "metadata": {
        "id": "GD97xBZTgyjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Google Colaboratory\n",
        "\n",
        "First of all, take a moment to look around and discover Google Colab if you haven't before! You can run the commands below to understand how much resources you're using and are still available. Then consider also that you can also connect you Google Drive for additional space or for easily loading your own files. Check out the [official tutorial](https://colab.research.google.com/) of the Google Colaboratory for more information.\n",
        "\n",
        "You can always reset the entire VM with \"*Runtime > Reset all runtime*\" in case of difficulty. Make also sure you're using the GPU or TPU in the same tab (\"*Runtime > Change runtime type*\")."
      ]
    },
    {
      "metadata": {
        "id": "AmbdpXfPhBeA",
        "colab_type": "code",
        "outputId": "43e7721e-8732-4701-c5fa-4daeba262767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "!free -m\n",
        "!df -h\n",
        "!nvidia-smi"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:          13022        2998        1992          67        8031       10763\n",
            "Swap:             0           0           0\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G   18G  323G   6% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "/dev/sda1       365G   22G  344G   6% /opt/bin\n",
            "tmpfs           6.4G  8.0K  6.4G   1% /var/colab\n",
            "shm             6.0G  4.0K  6.0G   1% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "Sat Dec 15 13:57:37 2018       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    71W / 149W |   1157MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M0VsdEjmhEY7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Questions to explore:\n",
        "\n",
        "*   How to connect your Google Drive with Google Colab?\n",
        "*   How to import a new notebook and save it to your GDrive?\n",
        "*  How to use files which are contained in your GDrive?\n",
        "\n",
        "Some tips here: https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n"
      ]
    },
    {
      "metadata": {
        "id": "tDvwfvb9hGvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the MNIST Benchamark"
      ]
    },
    {
      "metadata": {
        "id": "_SbJJIUenz07",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "in this section we load the common MNIST benchmark which we will use for our examples. We will take advantage of the *ContinualAI* calab scripts for easy loading of the MNIST images as numpy tensors:"
      ]
    },
    {
      "metadata": {
        "id": "7VdJOCNYbYu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S5RB5PMdbDNx",
        "colab_type": "code",
        "outputId": "0cd59074-ac25-4162-9aee-69a0e7608ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ContinualAI/colab.git continualai/colab"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'continualai/colab' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BD0nAldMbD5O",
        "colab_type": "code",
        "outputId": "64293949-58ea-4d8d-9673-582dec111381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from continualai.colab.scripts import mnist\n",
        "mnist.init()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S4cSFTbcbKeb",
        "colab_type": "code",
        "outputId": "4ffec9dd-65cf-4f2e-fdd8-5573591137a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "x_train, t_train, x_test, t_test = mnist.load()\n",
        "\n",
        "print(\"x_train dim and type: \", x_train.shape, x_train.dtype)\n",
        "print(\"t_train dim and type: \", t_train.shape, t_train.dtype)\n",
        "print(\"x_test dim and type: \", x_test.shape, x_test.dtype)\n",
        "print(\"t_test dim and type: \", t_test.shape, t_test.dtype)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train dim and type:  (60000, 1, 28, 28) float32\n",
            "t_train dim and type:  (60000,) uint8\n",
            "x_test dim and type:  (10000, 1, 28, 28) float32\n",
            "t_test dim and type:  (10000,) uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5h1yw26KrRUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us take a look at these images:"
      ]
    },
    {
      "metadata": {
        "id": "A1PMSnyNbNxr",
        "colab_type": "code",
        "outputId": "5507c34e-72ee-4d9e-e520-9baa98808ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "cell_type": "code",
      "source": [
        "f, axarr = plt.subplots(2,2)\n",
        "axarr[0,0].imshow(x_train[1, 0], cmap=\"gray\")\n",
        "axarr[0,1].imshow(x_train[2, 0], cmap=\"gray\")\n",
        "axarr[1,0].imshow(x_train[3, 0], cmap=\"gray\")\n",
        "axarr[1,1].imshow(x_train[4, 0], cmap=\"gray\")\n",
        "np.vectorize(lambda ax:ax.axis('off'))(axarr);"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAFLCAYAAADiejquAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f34387de780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7ajRYeYGBWm8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Common Constants"
      ]
    },
    {
      "metadata": {
        "id": "H5neMaipojPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can move on and define some common constants that we will share across the DL framework experiments:"
      ]
    },
    {
      "metadata": {
        "id": "UZwum8trBbJf",
        "colab_type": "code",
        "outputId": "4a63bed6-f1c2-41a0-c128-9776a35fd60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# we will use time to measure speed\n",
        "import time\n",
        "\n",
        "# number of classes in the MNIST dataset\n",
        "num_class = 10\n",
        "\n",
        "# number of epochs we will use for each training\n",
        "n_epochs = 2\n",
        "\n",
        "# mini-batch size for SGD\n",
        "minibatch_size = 100\n",
        "\n",
        "# Iterations for epoch for the two sets\n",
        "tr_it_for_epoch = t_train.shape[0] // minibatch_size\n",
        "te_it_for_epoch = t_test.shape[0] // minibatch_size\n",
        "print(\"train iterations: \", tr_it_for_epoch)\n",
        "print(\"test iterations: \", te_it_for_epoch)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train iterations:  600\n",
            "test iterations:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDYuMadt28ET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a ConvNet with Caffe\n",
        "\n",
        "Let us focus on *Caffe*. First of all let us install the library. Luckily enough for Ubuntu (>= 17.04) there is a packaged version we can install simply with:"
      ]
    },
    {
      "metadata": {
        "id": "Nv28V1_g1jeB",
        "colab_type": "code",
        "outputId": "c0aeca8b-b90a-4882-8acb-43435305414a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "# !apt install -y caffe-cpu\n",
        "!apt install -y caffe-cuda"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "caffe-cuda is already the newest version (1.0.0-6build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "333yU5z36lHw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us now import the library and check the version:"
      ]
    },
    {
      "metadata": {
        "id": "K1BEnLBJ6NRh",
        "colab_type": "code",
        "outputId": "57b49229-1280-43c3-a53b-5752271a405c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import caffe\n",
        "caffe.__version__\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "8sKZwmcTr2TO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can set the hardware type:"
      ]
    },
    {
      "metadata": {
        "id": "WIZ-69f76pwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#caffe.set_mode_cpu()\n",
        "caffe.set_device(0)\n",
        "caffe.set_mode_gpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsLmtuS2sMd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great! Now that we have caffe imported and configured, we can focus on the definition of our ConvNet and the training/testing procedures. The easiest way to define the network structure and the opimization parameters is to define two separate prototxts files. In this case we I have already prepared the net.prototxt and solver.prototxt files which we can import front the *ContinualAI-colab* toolchain:"
      ]
    },
    {
      "metadata": {
        "id": "7UaKFXZTT1yL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp continualai/colab/extras/net.prototxt .\n",
        "!cp continualai/colab/extras/solver.prototxt .\n",
        "solver_name = \"solver.prototxt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3H7t8awnaax3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before moving on let's visualize them with the awesome netscope tool: \n",
        "\n",
        "*   [net.prototxt](http://ethereon.github.io/netscope/#/gist/fbc84e148391c5bd953a5ec7d613b0f9)\n",
        "*   [solver.prototxt](https://gist.github.com/vlomonaco/82dbff5eab77e146b489d27a5cd5923f)"
      ]
    },
    {
      "metadata": {
        "id": "i7AZnMtEw1_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can define our test method:"
      ]
    },
    {
      "metadata": {
        "id": "juaaT4w2Kexo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(net, x, y, test_iters, test_batch_size):\n",
        "    \"\"\" test the trained net \"\"\"\n",
        "\n",
        "    acc = 0\n",
        "    loss = 0\n",
        "    for it in range(test_iters):\n",
        "        if it % 100 == 1: print(\"+\", end=\"\", flush = True)\n",
        "        start = it * test_batch_size\n",
        "        end = (it + 1) * test_batch_size\n",
        "        net.blobs['data'].data[...] = x[start:end]\n",
        "        net.blobs['label'].data[...] = y[start:end]\n",
        "        \n",
        "        blobs = net.forward([\"accuracy\", \"loss\"])\n",
        "        acc += blobs[\"accuracy\"]\n",
        "        loss += blobs[\"loss\"]\n",
        "\n",
        "    return acc / test_iters, loss / test_iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBejsywww7MJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the solver and start the training procedure:"
      ]
    },
    {
      "metadata": {
        "id": "nouIQYHMbo4t",
        "colab_type": "code",
        "outputId": "7dd864d7-eccf-424f-a5bf-f036ada6dcc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "solver = caffe.get_solver(solver_name)\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(\"Epoch\", epoch, \" \", end=\"\")\n",
        "  for it in range(tr_it_for_epoch):\n",
        "\n",
        "    if it % 100 == 1: print(\".\", end=\"\", flush=True)   \n",
        "    start = it * minibatch_size\n",
        "    end = (it + 1) * minibatch_size\n",
        "    solver.net.blobs['data'].data[...] = x_train[start:end]\n",
        "    solver.net.blobs['label'].data[...] = t_train[start:end]\n",
        "    solver.step(1)\n",
        "\n",
        "  train_acc, train_loss = test(solver.test_nets[0], x_train, t_train,\n",
        "                     tr_it_for_epoch, minibatch_size)\n",
        "  test_acc, _ = test(solver.test_nets[0], x_test, t_test,\n",
        "                     te_it_for_epoch, minibatch_size)\n",
        "  print(\"  Train loss: %.4f  Train acc: %.2f %%  Test acc: %.2f %%\" %\n",
        "        (train_loss, train_acc * 100, test_acc * 100))\n",
        "\n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 0  ......+++++++  Train loss: 0.0988  Train acc: 96.82 %  Test acc: 96.65 %\n",
            "Epoch 1  ......+++++++  Train loss: 0.0451  Train acc: 98.54 %  Test acc: 98.13 %\n",
            "---------------------------------------------\n",
            "120000 patterns (112.29 sec.) -> 1068.70 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "divl7TK36uPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### [Extra] Define model and solver from Python\n",
        "\n",
        "Of course is it possible to define the network directly in Python as shown below. However we leave this part as optional for the reader to explore."
      ]
    },
    {
      "metadata": {
        "id": "U3du0nJhBFtQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from caffe import layers as L\n",
        "from caffe import params as P\n",
        "from caffe.proto import caffe_pb2\n",
        "from google.protobuf import text_format\n",
        "\n",
        "def get_net(num_classes=10, train_mb_size=100):\n",
        "    \"\"\" Define net and return it as String \"\"\"\n",
        "\n",
        "    net = caffe.NetSpec()\n",
        "\n",
        "    net.data = L.Input(\n",
        "        shape=[dict(dim=[train_mb_size, 1, 28, 28, ])], ntop=1,\n",
        "        include=dict(phase=caffe.TRAIN)\n",
        "    )\n",
        "    net.test_data = L.Input(\n",
        "        shape=[dict(dim=[100, 1, 28, 28, ])], ntop=1,\n",
        "        include=dict(phase=caffe.TEST)\n",
        "    )\n",
        "    net.label = L.Input(\n",
        "        shape=[dict(dim=[train_mb_size])], ntop=1,\n",
        "        include=dict(phase=caffe.TRAIN)\n",
        "    )\n",
        "    net.test_label = L.Input(\n",
        "        shape=[dict(dim=[100])], ntop=1,\n",
        "        include=dict(phase=caffe.TEST)\n",
        "    )\n",
        "    net.conv1 = L.Convolution(\n",
        "        net.data, kernel_size=5,\n",
        "        num_output=32, param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "    net.relu1 = L.ReLU(net.conv1, in_place=True)\n",
        "\n",
        "    net.conv2 = L.Convolution(\n",
        "        net.relu1, kernel_size=5,\n",
        "        num_output=32, param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "    net.relu2 = L.ReLU(net.conv2, in_place=True)\n",
        "\n",
        "    net.fc1 = L.InnerProduct(\n",
        "        net.relu2, num_output=500,\n",
        "        param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "    net.relu3 = L.ReLU(net.fc1, in_place=True)\n",
        "\n",
        "    net.out = L.InnerProduct(\n",
        "        net.fc1, num_output=num_classes,\n",
        "        param=[dict(lr_mult=1), dict(lr_mult=2)],\n",
        "        weight_filler=dict(type='xavier'),\n",
        "        bias_filler=dict(type='constant')\n",
        "    )\n",
        "\n",
        "    net.loss = L.SoftmaxWithLoss(net.out, net.label)\n",
        "    \n",
        "    net.accuracy = L.Accuracy(\n",
        "        net.out, net.test_label, include=dict(phase=caffe.TEST)\n",
        "    )\n",
        "\n",
        "    proto = str(net.to_proto())\n",
        "    proto = proto.replace('test_data', 'data').replace('test_label', 'label')\\\n",
        "        .replace('test_target', 'target')\n",
        "    \n",
        "    return proto\n",
        "\n",
        "def get_solver( net, base_lr, random_seed=1, lr_policy=\"step\", gamma=0.1,\n",
        "    stepsize=100000000, momentum=0.9, weight_decay=0.0005, test_iter=0,\n",
        "    test_interval=1000, display=20, solver_mode=caffe_pb2.SolverParameter.GPU):\n",
        "    \"\"\" Define solver and return it as String \"\"\"\n",
        "\n",
        "    solver_config = caffe_pb2.SolverParameter()\n",
        "\n",
        "    solver_config.random_seed = random_seed\n",
        "    solver_config.test_iter.append(1)\n",
        "    solver_config.test_interval = 1\n",
        "    solver_config.net = net\n",
        "    solver_config.base_lr = base_lr\n",
        "    solver_config.lr_policy = lr_policy\n",
        "    solver_config.gamma = gamma\n",
        "    solver_config.stepsize = stepsize\n",
        "    solver_config.momentum = momentum\n",
        "    solver_config.weight_decay = weight_decay\n",
        "    solver_config.snapshot_format = caffe_pb2.SolverParameter.HDF5\n",
        "    solver_config.solver_mode = solver_mode\n",
        "\n",
        "    solver_config = text_format.MessageToString(\n",
        "        solver_config, float_format='.6g'\n",
        "    )\n",
        "\n",
        "    return solver_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9B6sNMJrWz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_name = \"net.prototxt\"\n",
        "\n",
        "with open(net_name, \"w\") as wf:\n",
        "  wf.write(get_net())\n",
        "  \n",
        "with open(solver_name, \"w\") as wf:\n",
        "  wf.write(get_solver(net_name, base_lr=0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXpmO6SvDugP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Questions to explore:\n",
        "\n",
        "*   How to recover the weights of a particular layer?\n",
        "*   How to get the activations of a particular layer?\n",
        "*  How to cast a classifier into a Fully Convolutional Network?\n",
        "\n",
        "Some tips here: https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb\n"
      ]
    },
    {
      "metadata": {
        "id": "6fJMJm8j9av0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a ConvNet with Tensorflow\n",
        "\n",
        "Let us move to the second framework we are considering: *Tensorflow*. We don't need to install it since it's already pre-loaded in Google Colaboratory (guess why! :'D). So let us start by importing it and checking the version:"
      ]
    },
    {
      "metadata": {
        "id": "uLf5ypBbqXC8",
        "colab_type": "code",
        "outputId": "39493e0b-2528-4b18-e5ae-266cddd2a7fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mW3BN4qPxvus",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we can define directly the network structure using the tf.layers API:"
      ]
    },
    {
      "metadata": {
        "id": "eBpfVAMncKN_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape=[minibatch_size, 1, 28, 28])\n",
        "t = tf.to_int64(tf.placeholder(tf.int32, shape=[minibatch_size]))\n",
        "\n",
        "# First Convolutional Layer\n",
        "x_image = tf.reshape(x, [-1,28,28,1])\n",
        "conv1 = tf.layers.conv2d(\n",
        "    x_image, 32, 5, strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        ")\n",
        "\n",
        "# Second Convolutional Layer\n",
        "conv2 = tf.layers.conv2d(\n",
        "    conv1, 32, 5, strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        ")\n",
        "pool2 = tf.layers.flatten(conv2)\n",
        "\n",
        "# Densely Connected Layer\n",
        "fc1 = tf.layers.dense(pool2, 500, name=\"fc1\", activation=tf.nn.relu)\n",
        "\n",
        "# Output Layer\n",
        "y_logits = tf.layers.dense(fc1, num_class, name=\"logits\")\n",
        "\n",
        "# Train and Evaluate the Model\n",
        "cross_entropy = tf.reduce_mean(\n",
        "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_logits, labels=t)\n",
        ")\n",
        "optim = tf.train.MomentumOptimizer(1e-2, momentum=0.9)\n",
        "train_step = optim.minimize(cross_entropy)\n",
        "correct_prediction = tf.equal(tf.argmax(y_logits,1), t)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "loss = tf.reduce_mean(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKfPKyiIx23V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As for caffe we can define the *test* function:"
      ]
    },
    {
      "metadata": {
        "id": "7iaAyAl5-CL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(x_set, y_set, test_iters, test_batch_size):\n",
        "    \"\"\" testing set accuracy: can be used for train and test\"\"\"\n",
        "    \n",
        "    accuracy_sum = 0.0\n",
        "    for it in range(test_iters):\n",
        "        if it % 100 == 1: print(\"+\", end=\"\", flush = True)\n",
        "        start = it * test_batch_size\n",
        "        end = (it + 1) * test_batch_size\n",
        "        accuracy_sum += sess.run(\n",
        "            fetches=accuracy,\n",
        "            feed_dict={x: x_set[start:end], t: y_set[start:end]}\n",
        "        )\n",
        "      \n",
        "    return accuracy_sum / test_iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h1wLJeyzx7sl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And then we can use the computational graph just created within an interactive session:"
      ]
    },
    {
      "metadata": {
        "id": "HkN-_fBuA2xY",
        "colab_type": "code",
        "outputId": "2696004f-73f3-4398-84bc-631f7a42fce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "train_loss = 0\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch\", epoch, \" \", end=\"\")\n",
        "    for it in range(tr_it_for_epoch):\n",
        "        if it % 100 == 1: print(\".\", end=\"\", flush = True)\n",
        "        start = it * minibatch_size\n",
        "        end = (it + 1) * minibatch_size\n",
        "        batch_loss, _ = sess.run(\n",
        "            fetches=[loss, train_step],\n",
        "            feed_dict={x: x_train[start:end], t: t_train[start:end]}\n",
        "        )\n",
        "        train_loss += batch_loss\n",
        "\n",
        "    train_loss = train_loss / tr_it_for_epoch\n",
        "    train_acc = test(\n",
        "        x_train, t_train, tr_it_for_epoch, minibatch_size,\n",
        "    )  \n",
        "    test_acc = test(\n",
        "        x_test, t_test, te_it_for_epoch, minibatch_size,\n",
        "    )\n",
        "\n",
        "    print(\"  Train loss: %.4f  Train acc: %.2f %%  Test acc: %.2f %%\" %\n",
        "        (train_loss, train_acc * 100, test_acc * 100))\n",
        "    \n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 0  ......+++++++  Train loss: 0.2559  Train acc: 97.60 %  Test acc: 97.40 %\n",
            "Epoch 1  ......+++++++  Train loss: 0.0604  Train acc: 98.26 %  Test acc: 97.80 %\n",
            "---------------------------------------------\n",
            "120000 patterns (20.27 sec.) -> 5920.06 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ATgmZZk_D1v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### [Extra] Keras API"
      ]
    },
    {
      "metadata": {
        "id": "2lJFRqwkyTz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As an bonus we leave to the reader the same implementation but using the keras API. Very easy, isn't it? But remember, with great abstraction power comes great responsibility..."
      ]
    },
    {
      "metadata": {
        "id": "BIxSi7BG9aUP",
        "colab_type": "code",
        "outputId": "00003012-14fc-4e31-afc6-fdaf7253a7f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(\n",
        "      32, (5, 5), strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        "  ),\n",
        "  tf.keras.layers.Conv2D(\n",
        "      32, (5, 5), strides=(1, 1), padding='valid', activation=tf.nn.relu\n",
        "  ),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(num_class, activation=tf.nn.softmax)\n",
        "])\n",
        "optim = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optim,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "\n",
        "model.fit(x_train, t_train, batch_size=minibatch_size, epochs=n_epochs)\n",
        "model.evaluate(x_test, t_test)\n",
        "\n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0366 - acc: 0.9888\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0252 - acc: 0.9920\n",
            "10000/10000 [==============================] - 1s 98us/step\n",
            "---------------------------------------------\n",
            "120000 patterns (17.05 sec.) -> 7040.14 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kKALvxrjFKWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Questions to explore:\n",
        "\n",
        "*   What happens if you comment the first line of code?\n",
        "*   What if you also change the convolution padding from \"valid\" to \"same\"?\n",
        "\n",
        "Some tips here: https://keras.io/layers/convolutional/\n"
      ]
    },
    {
      "metadata": {
        "id": "yqPN1d1B7CcH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a ConvNet with PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "hlT-5HvCy6x_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us delve now into the third and last framework we will consider: Pytorch. First of all let us install it and import it."
      ]
    },
    {
      "metadata": {
        "id": "JGRTfFMx7KAQ",
        "colab_type": "code",
        "outputId": "3f53f442-5093-4d84-b318-81388541862b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' #'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print('Platform:', platform, 'Accelerator:', accelerator)\n",
        "\n",
        "!pip install --upgrade --force-reinstall -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Platform: cp36-cp36m Accelerator: cu80\n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5b6a8000 @  0x7f19cfe102a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[31mjupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.15 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement six~=1.11.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
            "Torch 0.4.0 CUDA 8.0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GfSqaJK57MQI",
        "colab_type": "code",
        "outputId": "25242a23-44b2-4fe2-9fe2-c0b83cb24681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "BilxjAkF8q9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# switch to False to use CPU\n",
        "use_cuda = True\n",
        "\n",
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\");\n",
        "torch.manual_seed(1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TR5Z55XN7Okg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rY3otyjd7TXG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Questions to explore:**\n",
        "\n",
        "*   What's new in Pythorch 0.4?\n",
        "\n",
        "Some tips here: https://pytorch.org/blog/pytorch-0_4_0-migration-guide/\n"
      ]
    },
    {
      "metadata": {
        "id": "VD96_qvWzRt-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great! So now we can define the network and the independent forward function which can dynamically change depending on the input data:"
      ]
    },
    {
      "metadata": {
        "id": "s2N68j127TGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(512, 500)\n",
        "        self.fc2 = nn.Linear(500, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 512)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3t-0GmG0Pfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can define the test method as before:"
      ]
    },
    {
      "metadata": {
        "id": "c3BjalSR7QoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, device, x_test, t_test, test_iters, test_batch_size):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for it in range(test_iters):\n",
        "        if it % 100 == 1: print(\"+\", end=\"\", flush = True)\n",
        "        start = it * test_batch_size\n",
        "        end = (it + 1) * test_batch_size\n",
        "        with torch.no_grad():\n",
        "            x = torch.from_numpy(x_test[start:end])\n",
        "            y = torch.from_numpy(t_test[start:end]).long()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = model(x)\n",
        "            # sum up batch loss\n",
        "            test_loss += F.cross_entropy(output, y).item() \n",
        "            # get the index of the max log-probability\n",
        "            pred = output.max(1, keepdim=True)[1] \n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    return correct / len(t_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bpdEII350Wg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then finally define the model and start the training:"
      ]
    },
    {
      "metadata": {
        "id": "m-mvoCit7ycZ",
        "colab_type": "code",
        "outputId": "6aa7c338-1df7-4888-9419-04fe1dc242b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "model.train()\n",
        "\n",
        "t_start = time.time()\n",
        "print(\"Start Training\")\n",
        "train_loss = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(\"Epoch\", epoch, \" \", end=\"\")\n",
        "    train_loss = 0\n",
        "    for it in range(tr_it_for_epoch):\n",
        "        if it % 100 == 1: print(\".\", end=\"\", flush = True)\n",
        "        start = it * minibatch_size\n",
        "        end = (it + 1) * minibatch_size\n",
        "        x = torch.from_numpy(x_train[start:end])\n",
        "        y = torch.from_numpy(t_train[start:end]).long()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "    \n",
        "    train_loss = train_loss / tr_it_for_epoch\n",
        "    train_acc = test(\n",
        "        model, device, x_train, t_train, tr_it_for_epoch, minibatch_size,\n",
        "    )  \n",
        "    test_acc = test(\n",
        "        model, device, x_test, t_test, te_it_for_epoch, minibatch_size,\n",
        "    )\n",
        "\n",
        "    print(\"  Train loss: %.4f  Train acc: %.2f %%  Test acc: %.2f %%\" %\n",
        "        (train_loss, train_acc * 100, test_acc * 100))\n",
        "    \n",
        "t_elapsed = time.time()-t_start\n",
        "print(\"---------------------------------------------\")\n",
        "print ('%d patterns (%.2f sec.) -> %.2f patt/sec' % \n",
        "       (x_train.shape[0]*n_epochs, t_elapsed, \n",
        "        x_train.shape[0]*n_epochs / t_elapsed))\n",
        "print(\"---------------------------------------------\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "Epoch 0  ......+++++++  Train loss: 0.3784  Train acc: 97.12 %  Test acc: 97.43 %\n",
            "Epoch 1  ......+++++++  Train loss: 0.0778  Train acc: 98.06 %  Test acc: 98.05 %\n",
            "---------------------------------------------\n",
            "120000 patterns (8.33 sec.) -> 14403.05 patt/sec\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V0zANlq470iZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wow! ~98% accuracy in such a short time.\n",
        "\n",
        "**Questions to explore:**\n",
        "\n",
        "*   Can you find a better parametrization to improve the final accuracy?\n",
        "*   Can you change the network architecture to improve the final accuracy?\n",
        "*   Can you achieve the same performances with a smaller architecture?\n",
        "*   What's the difference in accuracy if you change convolutions with fully connected layers?\n",
        "* Can you improve the speed of the training for all the frameworks described above?\n",
        "* What are the pros and cons of each framework in this simple example?\n",
        "\n",
        "Some tips here: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354"
      ]
    },
    {
      "metadata": {
        "id": "M8t8lqRa0lFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This concludes our little tour of the thre most used open-source frameworks for Deep Learning. Please make a PR if you spot any error or you want to contribute to the **ContinualAI-Colab** project! :-) "
      ]
    },
    {
      "metadata": {
        "id": "70Cpx9db8G9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Copyright (c) 2018. Continual AI. All rights reserved. **\n",
        "\n",
        "See the accompanying LICENSE file in the GitHub repository for terms. \n",
        "\n",
        "*Date: 27-11-2018                                                             \n",
        "Author: Vincenzo Lomonaco                                                    \n",
        "E-mail: contact@continualai.org                                           \n",
        "Website: continualai.org*              "
      ]
    }
  ]
}